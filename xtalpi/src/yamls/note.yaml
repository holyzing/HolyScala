# pause
#   容器可以说是 pod 中一组容器的主进程，然后业务容器属于它的 “子进程“共享它的多种命名空间。（存储卷， 网络）

# ReplicationController：
#   确保容器应用的副本数始终保持在 用户定义的副本数，后续版本使用 ReplicationSet 取代

# ReplicaSet：
#   较 ReplicationController 而言支持集合式的 selector，集合式的选择器就是多个 label 确定一组容器。

# Deployment：
#   基于 replicaset 的支持更多pod 管理机制的 控制器。虽然副本控制器可以独立使用，但是不支持 Deployment
#   的滚动更新等机制，所以建议使用 Deployment 来维护一组容器副本的期望。
#   滚动更新的过程是创建一个新的 replicaSet，新的创建，旧的删除依次更替。
#   而且会保留旧的 replicaset（只是停用） 以便后续回滚。

# Horizontal Pod Autoscaling：
#   基于 relicaSet 对象进行定义，根据各 pod 的资源利用率进行pod扩展，当cpu或者内存（达到80%）的时候 会持续创建pod，
#   直至达到最大限定数，当利于率低于阈值的时候则回收pod。以求达到资源的水平自动扩展。（CPU 密集型任务）

# statefulset:
#   是为了解决有状态服务而定义的对象，（docker 容器主要解决的是无状态服务），一般应用场景有如下几例：
#       稳定的持久化存储，即POD重新调度后能访问到服务相同的持久化数据，（停用之前的数据）基于PVC 实现，
#       稳定的网络标志，即 Pod 重新调度后其PodName和HostName不变，基于 Headless Service （没有 Cluster IP 的service）实现。
#       有序部署，有序扩展，在Pod部署或者扩展的时候按照既定的顺序一次进行，并在最后一个pod run起来之前的所有pod都必须保持
#           runing 或者 ready 状态，基于 init container 实现 （mysql-apache（tomcat）-nginx）
#       有序收缩，有序删除
#   虽然是为了 维护具有状态的一组服务，但是 对于集群化的 mysql 的部署还是存在很多问题的？？？？？？？？？？？？？？？？？

# daemonset：
#   守护进程集
#   确保全部或者指定Node（打了污点的node）上运行一个Pod的副本。当有Node加入集群时，会自动为其创建一个Pod。当有Pod删除时，
#   自然会移除那个pod，（废话，所有pod都会被移除），删除 DaemonSet将会删除它创建的所有Pod。
#   典型用法：
#       运行集群存储 daemon，例如在每个Node 上运行 glusterd，ceph （分布式文件系统）
#       在每个Node上运行日志收集Daemon，例如fluentd，logstash，Kibana （数据收集）
#       在每个Node上运行监控 Daemon，例如 Prometheus Node Exporter + Grafana（监控程序）
#   PS：一个Node上运行好几个不同的 POD 1：一个pod中多个容器，使用一个DaemonSet 2.多个daemonset来

# Job：
#   负责批处理任务，即仅执行一次的任务，它保证批处理任务的一个或多个Pod ”成功结束”，而且可以设置任务
#   正常退出次数，达到那个次数之后才会回收Pod
#   cron job：管理基于时间的 Job（定时任务，给定时间点和周期执行）

# service：
#   暴露方式：NodePort 和 ClusterIP （ingress）
#   一组Pod（具有相同标签的Pod），使用 round ribbon算法 来轮询一组Pod进行请求转发。
#   适合维护一组特征变化（ip，name）但业务相同的Pod，
#   业务举例：service（squid（缓存））--> service(一组 apache pod)-->statefulSet(mysql)
#   显然service 具有负载均衡的功能，默认是根据kube-proxy的负载均衡策略，其实现是 基于 ipvs 的调度，
#   使用 ipvsadm -Ln 可查看负载规则

# ----------------------------------------------------------------------------------------------------------------------

# TODO Docker 的网络模型详细了解 ？？？？？？？？？？？

# nat：      虚拟机与主机连接到同一虚拟交换机，该交换机具有 nat的功能。
# bridge：   桥接是不需要虚拟交换机的，虚拟机是直接链路到物理网卡上的。
# host-only：主机与虚拟机一同连接一个虚拟交换机，该交换机不具备 nat转发的功能

# 交换机 路由器 网卡

# 负载均衡器：nginx lvs HAProxy

# LVS （Linux Virtual Server）：
#   通过LVS提供的负载均衡技术和Linux操作系统实现一个高性能，高可用的服务器群集。
#   IPVS 可以理解为就是 LVS， ipvsadm 是其命令行工具。

# flannel：overlay network
#   覆盖网络工具，给 每台宿主机（物理机）分配一个子网的方式为容器提供虚拟网络。基于 Linux Tun/TAP,
#   在win上安装docker 会有一个 TAP-Windows的工具被安装。
#   作为网络规划服务，其使用UDP封装IP包来创建overlay网络，并借助etcd维护网络的分配情况。
#
#   让集群中的不同节点主机创建的Docker 容器都具有集群唯一的虚拟IP地址。 而且它还能在这些IP 地址之间
#   建立一个覆盖网络（overlay Network），通过这个覆盖网络，将数据包原封不动的传递到目标容器中。
#   每个节点都会构建一个 flannel 容器进行该节点覆盖网络的同步并与 etcd 交换信息，
#   类似的服务工具还有 calico

# OVS （open vswitch）
#   一个高质量的，多层虚拟交换机（网络分层的层）让大规模网络自动化编程扩展，并仍支持标准的管理借口和协议：NetFlow，SFlow 等。
#   虚拟交换就是利用软件的方式形成交换部件，所以也叫软件交换机。

# Tun（IP Tunneling）
#   IP隧道技术主要用于移动主机和虚拟私有网络 （Virtual Private Network），
#   在其中隧道都是静态建立的，隧道一端有一IP地址，令一端也有唯一的ip地址。

# 网络模型：ovs + tun ==> flannel
#   kubernetes的网络模型假定了所有的Pod都在一个可以直接联通的扁平的网络空间中，这在Google的 Google Compute Engine 中实现了的。
#   但是在我们自己的集群里需要我们自己去实现这样的一个网络模型，去让集群内的所有容器之间网络互通。
#   kubernetes 提供了 CNI（CSI CRI） 让使用者去实现。
#
#   同一个 pod 内的容器共享基础容器 infrastructure pause 的网络栈（端口）命名空间，通过 lo（回环网卡） 即可进行互相访问。
#   同一个节点中的不同pod之间直接通过docker0网桥进行装发，不需要经过flannel。
#   不同节点上的各 Pod 之间的通讯：docker0的网段与宿主机物理网卡是属于两个网段，只能借助第三方关联进行数据包转发，
#       也就是 overlay Network （k8s中一般通过 flannel实现）
#       Pod 通过 vethpari与docker0连接，flannel0会使用钩子函数抓取docker0的数据包，flanneld对数据报文进行一定规则的
#       封装之后查询etcd路由表 进行物理机间内 flanneld服务端口之间的路由转发，逐层（2层）进行解读，直到到达目标POD
#   Pod到外网：Pod向外网发送请求，查找路由表，装发数据包到宿主机的网卡，宿主网卡完成路由选择后，iptables执行Masquerade，
#      n. 掩藏，掩饰；伪装，化装；欺骗；化妆舞会 v. 伪装，化装
#      冒充把源IP更改为宿主网卡的IP，然后向外网服务器发送请求，也就是 SNAT转换
#      (外网的响应到物理网卡之后，物理网卡会经过请求头进行解析到对应 pod ??)
#   外网到POD：通过 service 的网络（NodePort）访问（# LATER 后边会详细讲解）
#   Pod 与 Service 之间的通讯：目前基于性能考虑，全部为 IPtables 维护和装发（service网络），新版本引入了 lvs。
#
#   docker的网络中，可以为自己一个物理节点上的一组容器创建自己的一个网桥（指定名称和网段（局域网私有网段））
#   （可修改默认网桥 docker0，所谓网桥就是一个虚拟交换机）。
#
#   etcd的作用：存储管理 Flannel 可分配的IP地址段资源
#               监控 etcd 中的每个Pod的实际地址，并在内存中简历维护 Pod 节点路由表。
#               etcd 必然是需要高可用的集群化部署，etcd本身提供了非常稳定的高可用的集群化解决方案
# ----------------------------------------------------------------------------------------------------------------------

# kubeadm （集群构建）：
#   源码包构建安装，该种安装方式下的各组件都是以独立进程运行的，一旦出现故障之后，没有直接的方式使其恢复。
#   好处是可以清晰的知道各组件进程是如何配置启动运行的。
#   官方安装工具kubeadm，很多自动化安装工具都是 引用kubeadm 进行构建的。
#   kubeadm 初始化集群master的时候 默认会从 GCE 中拉取镜像
#
#   软路由：koolshare
#       国产的软件，使用软路由的原因是 为了容器可以翻墙，从而进行科学上网，
#       主要是为了访问 GCE（google 云），因为kubeadm 的镜像是由google 官网发布的。

#   操作系统：centos7 （4.4内核以上，支持绝大多数名称空间）
#        大型环境中让

#   镜像仓库：harbor
#       使用 公司的 harbor，但是要自己跟着构建一遍。

#   关闭swap 分区：防止将pod安装在虚拟内存中（使用 ingress 可以排除掉该种隐患，但是生产环境还是建议关闭）
#   关闭Selinux：

#   overlay2: 支持128层的镜像构建

#   kubectl 只能在master 节点执行 ??????
#   k8s 的 docker 可以直接pull 指定harbor的 公开的镜像而不用登录

#   docker ps -a -q

# kube-proxy：
#   解决 pod 和 service 之间的网络通讯方式
#
# kubelet： 需要和容器接口进行交互，启动容器，而以kubeadm安装的 k8s 组件服务都是
#           以POD（容器）的形式运行存在的，所以 kubelet 存在的一个意义就是启动 k8s 集群。
# ----------------------------------------------------------------------------------------------------------------------